{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewbrkich/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/andrewbrkich/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/andrewbrkich/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from math import log\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Preprocess Data\n",
    "\n",
    "Kaggle Dataset:\n",
    "https://www.kaggle.com/datasets/abisheksudarshan/topic-modeling-for-research-articles?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"project_data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_wordnet_tag(tag):\n",
    "    tag_map = {\n",
    "        \"J\": nltk.corpus.wordnet.ADJ,\n",
    "        \"N\": nltk.corpus.wordnet.NOUN,\n",
    "        \"V\": nltk.corpus.wordnet.VERB,\n",
    "        \"R\": nltk.corpus.wordnet.ADV\n",
    "    }\n",
    "    return tag_map.get(tag[0].upper(), nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def get_tokens(text):\n",
    "    tokens = nltk.RegexpTokenizer(\"[\\w']+\").tokenize(text)\n",
    "    tokens = nltk.pos_tag(tokens)\n",
    "    tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word, get_wordnet_tag(tag)) for word, tag in tokens]\n",
    "    tokens = [word for word in tokens if word not in nltk_sw]\n",
    "    return tokens\n",
    "\n",
    "def get_corpus(docs):\n",
    "    return docs.apply(get_tokens)\n",
    "\n",
    "# docs must be a list of lists of words\n",
    "def get_stopwords(corpus, tfidf=False):\n",
    "    words = {}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            words[word] = words.get(word, {})\n",
    "            words[word][i] = (words[word].get(i, 0) + 1) if tfidf else 1\n",
    "\n",
    "    for word in words:\n",
    "        if tfidf:\n",
    "            tf = sum(words[word].values())\n",
    "            df = len(words[word].values())\n",
    "            tfidf = tf / df\n",
    "            words[word] = tfidf\n",
    "        else:\n",
    "            words[word] = len(words[word].values()) / len(corpus)\n",
    "    \n",
    "    s = pd.Series(words)\n",
    "    return s[s > .50].sort_values()\n",
    "\n",
    "def remove_corpus_stopwords(corpus):\n",
    "    sw = get_stopwords(corpus)\n",
    "    return corpus.apply(lambda tokens: [word for word in tokens if word not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[\"ABSTRACT\"]\n",
    "corpus = remove_corpus_stopwords(get_corpus(docs))\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "word_freq = [dictionary.doc2bow(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " 0.021*\"hamiltonians\" + 0.014*\"hamiltonian\" + 0.013*\"1\" + 0.012*\"n\" + 0.010*\"2\" + 0.009*\"k\" + 0.008*\"manifold\" + 0.008*\"equation\" + 0.008*\"result\" + 0.007*\"bound\" + 0.006*\"function\" + 0.006*\"problem\" + 0.006*\"show\" + 0.006*\"give\" + 0.006*\"solution\" + 0.006*\"p\" + 0.006*\"prove\" + 0.005*\"time\" + 0.005*\"case\" + 0.005*\"group\" \n",
      "\n",
      "1 \n",
      " 0.016*\"method\" + 0.014*\"model\" + 0.009*\"data\" + 0.008*\"sample\" + 0.008*\"algorithm\" + 0.008*\"propose\" + 0.008*\"network\" + 0.007*\"use\" + 0.006*\"problem\" + 0.006*\"base\" + 0.006*\"show\" + 0.005*\"learn\" + 0.005*\"result\" + 0.005*\"paper\" + 0.005*\"help\" + 0.005*\"performance\" + 0.004*\"system\" + 0.004*\"carlo\" + 0.004*\"monte\" + 0.004*\"time\" \n",
      "\n",
      "2 \n",
      " 0.013*\"0\" + 0.011*\"1\" + 0.010*\"mass\" + 0.009*\"2\" + 0.009*\"galaxy\" + 0.008*\"star\" + 0.006*\"5\" + 0.006*\"3\" + 0.006*\"find\" + 0.006*\"high\" + 0.005*\"10\" + 0.005*\"observation\" + 0.005*\"_\" + 0.005*\"present\" + 0.005*\"stellar\" + 0.004*\"gas\" + 0.004*\"low\" + 0.004*\"4\" + 0.004*\"model\" + 0.004*\"cluster\" \n",
      "\n",
      "3 \n",
      " 0.013*\"magnetic\" + 0.012*\"couple\" + 0.012*\"field\" + 0.011*\"momentum\" + 0.010*\"magnetocrystalline\" + 0.008*\"potential\" + 0.007*\"nature\" + 0.007*\"phase\" + 0.007*\"anisotropic\" + 0.007*\"result\" + 0.007*\"state\" + 0.007*\"crystallographic\" + 0.007*\"irreversible\" + 0.007*\"theory\" + 0.006*\"skx\" + 0.006*\"energy\" + 0.006*\"spin\" + 0.005*\"quantum\" + 0.005*\"lattice\" + 0.005*\"temperature\" \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=word_freq,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=4,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha=\"auto\",\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "[print(row[0], '\\n', row[1], '\\n') for row in lda_model.print_topics(num_words=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert topic word probabilities tuple into a dictionary for easy indexing\n",
    "def tuples_to_dict(list_of_tuples):\n",
    "    result = dict()\n",
    "    for key,value in list_of_tuples:\n",
    "        result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_statistics(lda_topic_results):\n",
    "    # convert topic word probs from lda_model.show_topics() to list of dictionaries\n",
    "    topic_word_probs = []\n",
    "    for topic in lda_topic_results: topic_word_probs.append(tuples_to_dict(topic[1]))\n",
    "    \n",
    "    # get list of unique words\n",
    "    all_words = []\n",
    "    for topic in topic_word_probs:\n",
    "        for key in topic.keys(): all_words.append(key)\n",
    "    all_words = (np.unique(all_words)).tolist()\n",
    "    \n",
    "    # create dataframe to hold statistics\n",
    "    df = pd.DataFrame(columns=['mean', 'stdev', 'num_topics'], index=all_words)\n",
    "    \n",
    "    for word in all_words:\n",
    "        vals = []\n",
    "        for topic_dict in topic_word_probs:\n",
    "            if word in topic_dict.keys(): vals.append(float(topic_dict[word]))\n",
    "            else: vals.append(float(0))\n",
    "        df.loc[word] = pd.Series({'mean': statistics.mean(vals), 'stdev': statistics.stdev(vals), 'num_topics': np.count_nonzero(vals)})\n",
    "    return df.sort_values(by=['num_topics'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "      <th>num_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give</th>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generalized</th>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas</th>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>galaxy</th>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean     stdev num_topics\n",
       "result       0.004872  0.003421        3.0\n",
       "time         0.002294  0.002686        2.0\n",
       "model        0.004539  0.006603        2.0\n",
       "problem      0.003108  0.003589        2.0\n",
       "show         0.002984  0.003452        2.0\n",
       "...               ...       ...        ...\n",
       "give         0.001534  0.003068        1.0\n",
       "generalized  0.001187  0.002375        1.0\n",
       "gas          0.001086  0.002171        1.0\n",
       "galaxy       0.002145  0.004291        1.0\n",
       "z            0.000899  0.001798        1.0\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not sure if mean and stdev are being calculated how he wants - currently I use the probabilities from the lda result to calculate their mean & stdev\n",
    "topic_word_statistics(lda_model.show_topics(num_words=25, formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code for removing values under a certain mean/stdev threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - topic cohesiveness using WordNet path_similarity\n",
    "\n",
    "def method_1(lda_topic_results):\n",
    "    result = {}\n",
    "    words_without_senses = []\n",
    "    \n",
    "    # convert topic word probs from lda_model.show_topics() to list of dictionaries\n",
    "    topic_word_probs = []\n",
    "    for topic in lda_topic_results: topic_word_probs.append(tuples_to_dict(topic[1]))\n",
    "    \n",
    "    # calculate cohesiveness score for each topic\n",
    "    for topic in range(len(topic_word_probs)):\n",
    "        min_senses = []\n",
    "        topic_words = topic_word_probs[topic].keys()\n",
    "        \n",
    "        # for each pair of words, calculate path similarity of each combination of senses for each word\n",
    "        word_combinations = itertools.combinations(topic_words, 2)\n",
    "        for word_pair in word_combinations:\n",
    "            sense_similarities = []\n",
    "            word1_senses = nltk.corpus.wordnet.synsets(str(word_pair[0]))\n",
    "            word2_senses = nltk.corpus.wordnet.synsets(str(word_pair[1]))\n",
    "            \n",
    "            if(len(word1_senses) == 0): words_without_senses.append(word_pair[0])\n",
    "            if(len(word2_senses) == 0): words_without_senses.append(word_pair[1])\n",
    "\n",
    "            for syn1 in word1_senses:\n",
    "                for syn2 in word2_senses:\n",
    "                    sense_similarities.append(syn1.path_similarity(syn2))\n",
    "            if len(sense_similarities) > 0: min_senses.append(min(sense_similarities))\n",
    "        result[\"Topic \" + str(topic)] = statistics.mean(min_senses)\n",
    "    print(\"Topic words omitted because no WordNet sense was found:\")\n",
    "    print(np.unique(words_without_senses).tolist())\n",
    "    return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic words omitted because no WordNet sense was found:\n",
      "['_', 'carlo', 'crystallographic', 'hamiltonian', 'hamiltonians', 'hmc', 'magnetocrystalline', 'skx']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Topic 0': 0.06164714244613044,\n",
       " 'Topic 1': 0.06368722392688668,\n",
       " 'Topic 2': 0.06754333609100402,\n",
       " 'Topic 3': 0.07356070916606322}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_1(lda_model.show_topics(num_words=25, formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2\n",
    "#things I havn't done: combination/permutations of all possible senses (i just picked the first of each word)\n",
    "\n",
    "def method_2(lda_topic_results):\n",
    "    \n",
    "    # convert topic word probs from lda_model.show_topics() to list of dictionaries\n",
    "    topic_word_probs = []\n",
    "    for topic in lda_topic_results: topic_word_probs.append(tuples_to_dict(topic[1]))\n",
    "\n",
    "    for topic in range(len(topic_word_probs)):\n",
    "        print(\"topic: \", topic)\n",
    "        topic_words = topic_word_probs[topic].keys()\n",
    "        topic_vals = topic_word_probs[topic].values()\n",
    "        word_senses = []\n",
    "\n",
    "        #get all the senses of each word\n",
    "        for word in topic_words:\n",
    "            senses = nltk.corpus.wordnet.synsets(str(word))\n",
    "            word_senses.append(senses)\n",
    "        # print(word_senses)\n",
    "\n",
    "        while len(word_senses) != 1:\n",
    "            #find least common ancestor for at least 1 sense of all words \n",
    "            #may need combinations/permutations library here to pick synsets randomly/all possibilities\n",
    "            words_to_look_up = []\n",
    "            for synsets in word_senses:\n",
    "                try:\n",
    "                    words_to_look_up.append(synsets[0])\n",
    "                except:\n",
    "                    #this word has no sysnsets\n",
    "                    x=1\n",
    "            print(\"Words to look up: \", words_to_look_up)\n",
    "\n",
    "            #find least common hypernym for words_to_look_up\n",
    "            #may need combonations/permutations here too\n",
    "            ancestors = []\n",
    "            for i in range(len(words_to_look_up) -1):\n",
    "                ancestors.append(words_to_look_up[i].lowest_common_hypernyms(words_to_look_up[i+1]))\n",
    "\n",
    "            print(\"Ancestors: \", ancestors)\n",
    "            #clean up ancestors, make only unique entries remain and get rid of blanks\n",
    "            unique_ancestors = []\n",
    "            for item in ancestors:\n",
    "                if item not in unique_ancestors:\n",
    "                    unique_ancestors.append(item)\n",
    "            res = list(filter(None, unique_ancestors))\n",
    "            print(\"Filtered list to use next iteration: \", res)\n",
    "\n",
    "            #set senses = filtered ancestors, and continue until only 1 item remains\n",
    "            word_senses = res\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "749418a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "Words to look up:  [Synset('one.n.01'), Synset('nitrogen.n.01'), Synset('two.n.01'), Synset('kelvin.n.01'), Synset('manifold.n.01'), Synset('equation.n.01'), Synset('consequence.n.01'), Synset('boundary.n.02'), Synset('function.n.01'), Synset('problem.n.01'), Synset('show.n.01'), Synset('give.n.01'), Synset('solution.n.01'), Synset('phosphorus.n.01'), Synset('prove.v.01'), Synset('time.n.01'), Synset('case.n.01'), Synset('group.n.01'), Synset('space.n.01'), Synset('generalize.v.01'), Synset('riemann.n.01'), Synset('number.n.01'), Synset('ten.n.01')]\n",
      "Ancestors:  [[Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('definite_quantity.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('substance.n.01')], [], [], [Synset('case.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [], [], [Synset('entity.n.01')], [Synset('abstraction.n.06')]]\n",
      "Filtered list to use next iteration:  [[Synset('abstraction.n.06')], [Synset('definite_quantity.n.01')], [Synset('entity.n.01')], [Synset('substance.n.01')], [Synset('case.n.01')]]\n",
      "Words to look up:  [Synset('abstraction.n.06'), Synset('definite_quantity.n.01'), Synset('entity.n.01'), Synset('substance.n.01'), Synset('case.n.01')]\n",
      "Ancestors:  [[Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('abstraction.n.06')]]\n",
      "Filtered list to use next iteration:  [[Synset('abstraction.n.06')], [Synset('entity.n.01')]]\n",
      "Words to look up:  [Synset('abstraction.n.06'), Synset('entity.n.01')]\n",
      "Ancestors:  [[Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')]]\n",
      "topic:  1\n",
      "Words to look up:  [Synset('method.n.01'), Synset('model.n.01'), Synset('data.n.01'), Synset('sample.n.01'), Synset('algorithm.n.01'), Synset('propose.v.01'), Synset('network.n.01'), Synset('use.n.01'), Synset('problem.n.01'), Synset('base.n.01'), Synset('show.n.01'), Synset('learn.v.01'), Synset('consequence.n.01'), Synset('paper.n.01'), Synset('aid.n.02'), Synset('performance.n.01'), Synset('system.n.01'), Synset('monte.n.01'), Synset('time.n.01'), Synset('trial.n.02'), Synset('idea.n.01'), Synset('two.n.01'), Synset('image.n.01')]\n",
      "Ancestors:  [[Synset('cognition.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('psychological_feature.n.01')], [], [], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [], [], [Synset('physical_entity.n.01')], [Synset('abstraction.n.06')], [Synset('event.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('event.n.01')], [Synset('psychological_feature.n.01')], [Synset('cognition.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')]]\n",
      "Filtered list to use next iteration:  [[Synset('cognition.n.01')], [Synset('abstraction.n.06')], [Synset('psychological_feature.n.01')], [Synset('entity.n.01')], [Synset('physical_entity.n.01')], [Synset('event.n.01')]]\n",
      "Words to look up:  [Synset('cognition.n.01'), Synset('abstraction.n.06'), Synset('psychological_feature.n.01'), Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('event.n.01')]\n",
      "Ancestors:  [[Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('abstraction.n.06')], [Synset('entity.n.01')]]\n",
      "Words to look up:  [Synset('abstraction.n.06'), Synset('entity.n.01')]\n",
      "Ancestors:  [[Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')]]\n",
      "topic:  2\n",
      "Words to look up:  [Synset('zero.n.02'), Synset('one.n.01'), Synset('mass.n.01'), Synset('two.n.01'), Synset('galaxy.n.01'), Synset('star.n.01'), Synset('five.n.01'), Synset('three.n.01'), Synset('discovery.n.03'), Synset('high.n.01'), Synset('ten.n.01'), Synset('observation.n.01'), Synset('present.n.01'), Synset('leading.s.01'), Synset('gas.n.01'), Synset('low.n.01'), Synset('four.n.01'), Synset('model.n.01'), Synset('bunch.n.01'), Synset('spectrum.n.01'), Synset('formation.n.01'), Synset('survey.n.01'), Synset('dark.n.01'), Synset('omega.n.01')]\n",
      "Ancestors:  [[Synset('digit.n.01')], [Synset('measure.n.02')], [Synset('measure.n.02')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('digit.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [], [], [Synset('natural_phenomenon.n.01')], [Synset('entity.n.01')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('group.n.01')], [Synset('arrangement.n.02')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')], [Synset('abstraction.n.06')]]\n",
      "Filtered list to use next iteration:  [[Synset('digit.n.01')], [Synset('measure.n.02')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('natural_phenomenon.n.01')], [Synset('group.n.01')], [Synset('arrangement.n.02')]]\n",
      "Words to look up:  [Synset('digit.n.01'), Synset('measure.n.02'), Synset('abstraction.n.06'), Synset('entity.n.01'), Synset('natural_phenomenon.n.01'), Synset('group.n.01'), Synset('arrangement.n.02')]\n",
      "Ancestors:  [[Synset('measure.n.02')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('group.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('measure.n.02')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('group.n.01')]]\n",
      "Words to look up:  [Synset('measure.n.02'), Synset('abstraction.n.06'), Synset('entity.n.01'), Synset('group.n.01')]\n",
      "Ancestors:  [[Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('abstraction.n.06')], [Synset('entity.n.01')]]\n",
      "Words to look up:  [Synset('abstraction.n.06'), Synset('entity.n.01')]\n",
      "Ancestors:  [[Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')]]\n",
      "topic:  3\n",
      "Words to look up:  [Synset('magnetic.a.01'), Synset('couple.n.01'), Synset('field.n.01'), Synset('momentum.n.01'), Synset('potential.n.01'), Synset('nature.n.01'), Synset('phase.n.01'), Synset('anisotropic.a.01'), Synset('consequence.n.01'), Synset('state.n.01'), Synset('irreversible.a.01'), Synset('theory.n.01'), Synset('energy.n.01'), Synset('spin.n.01'), Synset('quantum.n.01'), Synset('lattice.n.01'), Synset('temperature.n.01'), Synset('detect.v.01'), Synset('investigate.v.01'), Synset('order.n.01'), Synset('system.n.01'), Synset('change.n.01')]\n",
      "Ancestors:  [[], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('attribute.n.02')], [Synset('attribute.n.02')], [Synset('abstraction.n.06')], [], [], [Synset('physical_entity.n.01')], [], [], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('psychological_feature.n.01')], [Synset('cognition.n.01')], [Synset('abstraction.n.06')], [], [], [], [Synset('entity.n.01')], [Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')], [Synset('attribute.n.02')], [Synset('abstraction.n.06')], [Synset('physical_entity.n.01')], [Synset('psychological_feature.n.01')], [Synset('cognition.n.01')]]\n",
      "Words to look up:  [Synset('entity.n.01'), Synset('attribute.n.02'), Synset('abstraction.n.06'), Synset('physical_entity.n.01'), Synset('psychological_feature.n.01'), Synset('cognition.n.01')]\n",
      "Ancestors:  [[Synset('entity.n.01')], [Synset('abstraction.n.06')], [Synset('entity.n.01')], [Synset('entity.n.01')], [Synset('psychological_feature.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')], [Synset('abstraction.n.06')], [Synset('psychological_feature.n.01')]]\n",
      "Words to look up:  [Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('psychological_feature.n.01')]\n",
      "Ancestors:  [[Synset('entity.n.01')], [Synset('abstraction.n.06')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')], [Synset('abstraction.n.06')]]\n",
      "Words to look up:  [Synset('entity.n.01'), Synset('abstraction.n.06')]\n",
      "Ancestors:  [[Synset('entity.n.01')]]\n",
      "Filtered list to use next iteration:  [[Synset('entity.n.01')]]\n"
     ]
    }
   ],
   "source": [
    "method_2(lda_model.show_topics(num_words=25, formatted=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
